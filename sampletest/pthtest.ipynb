{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_80514/2822640057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m CONFIGS = {\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# \"sup_vitb8\": configs.get_b16_config(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;34m\"sup_vitb16_224\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_b16_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"sup_vitb16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_b16_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"sup_vitl16_224\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_l16_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configs' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "\"\"\"\n",
    "models for vits, borrowed from\n",
    "https://github.com/jeonsworld/ViT-pytorch/blob/main/models/modeling_resnet.py\n",
    "https://github.com/jeonsworld/ViT-pytorch/blob/main/models/modeling.py\n",
    "\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from turtle import forward\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "\n",
    "from ...configs import vit_configs as configs\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CONFIGS = {\n",
    "    # \"sup_vitb8\": configs.get_b16_config(),\n",
    "    \"sup_vitb16_224\": configs.get_b16_config(),\n",
    "    \"sup_vitb16\": configs.get_b16_config(),\n",
    "    \"sup_vitl16_224\": configs.get_l16_config(),\n",
    "    \"sup_vitl16\": configs.get_l16_config(),\n",
    "    \"sup_vitb16_imagenet21k\": configs.get_b16_config(),\n",
    "    \"sup_vitl16_imagenet21k\": configs.get_l16_config(),\n",
    "    \"sup_vitl32_imagenet21k\": configs.get_l32_config(),\n",
    "    'sup_vitb32_imagenet21k': configs.get_b32_config(),\n",
    "    'sup_vitb8_imagenet21k': configs.get_b8_config(),\n",
    "    'sup_vith14_imagenet21k': configs.get_h14_config(),\n",
    "    # 'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
    "}\n",
    "\n",
    "\n",
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states) # B, num_patches, head_size*num_head\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer) # B, num_head, num_patches, head_size\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) # B, num_head, num_patches, head_size\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # B, num_head, num_patches, num_patches\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores) # B, num_head, num_patches(query), num_patches(key)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer) # B, num_head, num_patches, head_size\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
    "                                         width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.hybrid:\n",
    "            x = self.hybrid_model(x)\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights\n",
    "\n",
    "    def forward_cls_layerwise(self, hidden_states):\n",
    "        # hidden_states: B, 1+n_patches, dim\n",
    "\n",
    "        if hidden_states.size(0) != 1:\n",
    "            raise ValueError('not support batch-wise cls forward yet')\n",
    "        \n",
    "        cls_embeds = []\n",
    "        cls_embeds.append(hidden_states[0][0])\n",
    "        for i,layer_block in enumerate(self.layer):\n",
    "            hidden_states, _ = layer_block(hidden_states)\n",
    "            if i < len(self.layer)-1:\n",
    "                cls_embeds.append(hidden_states[0][0])\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        cls_embeds.append(hidden_states[0][0])\n",
    "\n",
    "        cls_embeds = torch.stack(cls_embeds) # 12, dim\n",
    "        return cls_embeds\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "\n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "        return encoded, attn_weights\n",
    "    \n",
    "    def forward_cls_layerwise(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "\n",
    "        cls_embeds = self.encoder.forward_cls_layerwise(embedding_output)\n",
    "        return cls_embeds\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_type,\n",
    "        img_size=224, num_classes=21843, vis=False\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        config = CONFIGS[model_type]\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.head = Linear(config.hidden_size, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, vis=False):\n",
    "        x, attn_weights = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "\n",
    "        if not vis:\n",
    "            return logits\n",
    "        return logits, attn_weights # attn_weights: num_layers, B, num_head, num_patches, num_patches\n",
    "    \n",
    "    def forward_cls_layerwise(self, x):\n",
    "        cls_embeds = self.transformer.forward_cls_layerwise(x)\n",
    "        return cls_embeds\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "class StdConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "                        self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation (v2) bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        cout = cout or cin\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
    "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if (stride != 1 or cin != cout):\n",
    "            # Projection also with pre-activation according to paper.\n",
    "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
    "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.gn_proj(residual)\n",
    "\n",
    "        # Unit's branch\n",
    "        y = self.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.relu(self.gn2(self.conv2(y)))\n",
    "        y = self.gn3(self.conv3(y))\n",
    "\n",
    "        y = self.relu(residual + y)\n",
    "        return y\n",
    "\n",
    "    def load_from(self, weights, n_block, n_unit):\n",
    "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
    "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
    "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
    "\n",
    "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
    "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
    "\n",
    "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
    "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
    "\n",
    "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
    "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
    "\n",
    "        self.conv1.weight.copy_(conv1_weight)\n",
    "        self.conv2.weight.copy_(conv2_weight)\n",
    "        self.conv3.weight.copy_(conv3_weight)\n",
    "\n",
    "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
    "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
    "\n",
    "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
    "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
    "\n",
    "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
    "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
    "\n",
    "        if hasattr(self, 'downsample'):\n",
    "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
    "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
    "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
    "\n",
    "            self.downsample.weight.copy_(proj_conv_weight)\n",
    "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
    "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n",
    "\n",
    "\n",
    "class ResNetV2(nn.Module):\n",
    "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
    "\n",
    "    def __init__(self, block_units, width_factor):\n",
    "        super().__init__()\n",
    "        width = int(64 * width_factor)\n",
    "        self.width = width\n",
    "\n",
    "        # The following will be unreadable if we split lines.\n",
    "        # pylint: disable=line-too-long\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
    "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
    "                ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
    "                ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
    "                ))),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.root(x)\n",
    "        x = self.body(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 모델 경로 설정에 따라 다른 모델을 임포트\n",
    "from vit_backbones.vit import VisionTransformer  # 경로 수정이 필요할 수 있습니다.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_and_load_model(model_type, npz_path):\n",
    "    # 모델 인스턴스 생성\n",
    "    model = VisionTransformer(model_type, num_classes=-1)\n",
    "    model.to(device)\n",
    "\n",
    "    # npz 파일에서 가중치 로드\n",
    "    weights = np.load(npz_path)\n",
    "    state_dict = {key: torch.from_numpy(val) for key, val in weights.items()}\n",
    "    model.load_state_dict(state_dict, strict=False)  # strict를 False로 설정하여 완벽하게 일치하지 않아도 로드\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_and_apply_prompt(model, prompt_path):\n",
    "    # pth 파일 로드\n",
    "    prompt_data = torch.load(prompt_path, map_location=device)\n",
    "    # 프롬프트 적용\n",
    "    model.enc.transformer.prompt_embeddings.data = torch.from_numpy(prompt_data['shallow_prompt']).to(device)\n",
    "    if 'deep_prompt' in prompt_data:\n",
    "        model.enc.transformer.deep_prompt_embeddings.data = torch.from_numpy(prompt_data['deep_prompt']).to(device)\n",
    "\n",
    "# 모델, npz 파일 경로 및 pth 파일 경로 설정\n",
    "model_type = 'imagenet21k_ViT-B_16'\n",
    "npz_path = '/workspace/model/imagenet21k_ViT-B_16.npz'\n",
    "prompt_path = '/root/nlp/vpt_skill/output/has_size::small_(5_-_9_in)/seed30/lr0.002/run1/prompt_ep10.pth'\n",
    "\n",
    "# 모델 빌드 및 npz 가중치 로드\n",
    "model = build_and_load_model(model_type, npz_path)\n",
    "\n",
    "# pth 파일에서 프롬프트 로드 및 적용\n",
    "load_and_apply_prompt(model, prompt_path)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "print(\"Model is loaded with pre-trained weights and prompts, and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
